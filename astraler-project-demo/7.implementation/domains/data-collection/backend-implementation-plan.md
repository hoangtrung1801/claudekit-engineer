# Data Collection Backend Implementation Plan

> **Domain:** Data Collection
> **Status:** ğŸŸ¢ Mostly Complete (Refactoring Done)
> **Created:** December 13, 2024
> **Last Updated:** December 17, 2024
> **Progress:** 67/70 tasks completed (96%)
> **Priority:** P0 - Critical
> **Phase:** 2 (Data Engine) - Refactoring Complete

---

## âš ï¸ Scope Adjustments

| Feature | Status | Notes |
|---------|--------|-------|
| **YouTube Crawler** | ğŸŸ¡ Deferred to Phase 2.1 | Lower priority for MVP |
| **Google Play Store** | ğŸ”´ Backlog | SearchAPI does not support, need to find another provider |
| **Landing Page Crawler** | âœ… Included | Only extract social links (does not crawl content) |

---

## 1. Overview

This plan covers the backend implementation for **Data Collection** domain, including:
- BullMQ queue setup for job processing
- Apify adapter for social media scraping
- Store crawler (App Store only - Google Play deferred)
- Social channel & content crawlers
- Landing page crawler
- Scheduling system
- Error handling & rate limiting

**Reference Documents:**
- Domain Planning: `docs/5.planning-setup/domains/data-collection/domain-planning.md`
- Domain TDD: `docs/3.technical-design/domains/data-collection/domain-tdd.md`

**Estimated Duration:** 6 weeks  
**Dependencies:** Project Management (Competitors, Social Channels)

---

## 2. Prerequisites

- [ ] Project Management Backend completed
- [ ] Redis server running
- [ ] Apify API key configured
- [ ] Event emitter configured

---

## 3. API Endpoints Summary

| Method | Endpoint | Description | Auth |
|--------|----------|-------------|------|
| POST | `/api/crawl/force-refresh/:competitorId` | Force immediate crawl | Yes |
| GET | `/api/crawl/status/:jobId` | Get job status | Yes |
| GET | `/api/crawl/queue/stats` | Queue statistics | Yes (Admin) |

---

## 4. Implementation Tasks

### 4.1 Infrastructure Setup (DC-I-*)

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-I-001 | Install BullMQ and Redis dependencies | 1h | âœ… |
| DC-I-002 | Configure Redis connection in NestJS | 2h | âœ… |
| DC-I-003 | Create crawl-queue queue definition | 1h | âœ… |
| DC-I-004 | Setup Bull Board for monitoring | 2h | âœ… |
| DC-I-005 | Configure job options (retries, backoff) | 2h | âœ… |

### 4.2 Apify Adapter (DC-AP-*)

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-AP-001 | Create ApifyAdapter service | 4h | âœ… |
| DC-AP-002 | Implement Apify API authentication | 2h | âœ… |
| DC-AP-003 | Create response type definitions | 2h | âœ… |
| DC-AP-004 | Implement error handling wrapper | 2h | âœ… |
| DC-AP-005 | Implement actor run and wait | 2h | âœ… |

### 4.3 Store Crawler (DC-ST-*) - Apple App Store Only

> âš ï¸ **Note**: Google Play Store (DC-ST-003) moved to backlog - pending provider research.  
> âš ï¸ **Important**: Store Crawler must collect the same comprehensive metadata as Project Info crawler (description, rating, ratingsCount, bundleId, screenshots) for consistency.

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-ST-001 | Create StoreProcessor class | 2h | âœ… |
| DC-ST-002 | Implement App Store metadata fetch | 3h | âœ… |
| ~~DC-ST-003~~ | ~~Implement Play Store metadata fetch~~ | ~~3h~~ | ğŸ”´ Backlog |
| DC-ST-004 | Save competitor metadata to database (name, developer, icon, category) | 2h | âœ… |
| DC-ST-004A | Add description, rating, ratingsCount, bundleId fields to Competitor model | 1h | âœ… |
| DC-ST-004B | Update StoreProcessor to save description, rating, ratingsCount, bundleId | 1h | âœ… |
| DC-ST-005 | Create CompetitorScreenshot model and save screenshots | 2h | âœ… |
| DC-ST-006 | Fetch and save reviews | 3h | âœ… |
| DC-ST-007 | Fetch and save app updates | 2h | âœ… |

### 4.4 Social Channel Crawler (DC-SC-*)

> âš ï¸ **Note**: YouTube (DC-SC-003) moved to Phase 2.1 - lower priority for MVP.

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-SC-001 | Create SocialChannelProcessor class | 2h | âœ… |
| DC-SC-002 | Implement TikTok profile fetch | 3h | âœ… |
| ~~DC-SC-003~~ | ~~Implement YouTube channel fetch~~ | ~~3h~~ | ğŸŸ¡ Phase 2.1 |
| DC-SC-004 | Implement Instagram profile fetch | 3h | âœ… |
| DC-SC-005 | Implement Facebook page fetch | 3h | âœ… |
| DC-SC-006 | Save channel data and create snapshot | 2h | âœ… |
| DC-SC-007 | Emit event for content crawl | 1h | âœ… |

### 4.5 Social Content Crawler (DC-CC-*)

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-CC-001 | Create SocialContentProcessor class | 2h | âœ… |
| DC-CC-002 | Implement video list fetch (all platforms) | 4h | âœ… |
| DC-CC-003 | Upsert videos to database | 2h | âœ… |
| DC-CC-004 | Create video snapshots | 1h | âœ… |
| DC-CC-005 | Implement posts fetch | 3h | âœ… |
| DC-CC-006 | Upsert posts to database | 2h | âœ… |
| DC-CC-007 | Emit video.created event | 1h | âœ… |

### 4.6 Landing Page Crawler (DC-LP-*) - Social Link Extraction Only

> âš ï¸ **Scope**: Only extract social links from landing page. DOES NOT crawl full content/copywriting.

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-LP-001 | Setup Playwright service | 2h | âœ… (using fetch) |
| DC-LP-002 | Create LandingPageProcessor | 2h | âœ… |
| DC-LP-003 | Implement social link extraction (TikTok, YouTube, IG, FB, X) | 3h | âœ… |
| DC-LP-004 | Emit social.discovered event | 1h | âœ… |

### 4.7 Ads Library Crawler (DC-ADS-*) â€“ Initial Implementation (Keywords-Centric)

> âœ… **Completed**: December 15, 2024  
> **Original Scope**: Crawl ads from Meta, TikTok, and Google Ads Library using ASO keywords from Project's Keyword table.

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-ADS-001 | Create AdsLibraryProcessor class | 2h | âœ… |
| DC-ADS-002 | Add ADS_LIBRARY_CRAWL to CrawlJobType enum | 0.5h | âœ… |
| DC-ADS-003 | Create AdsLibraryCrawlJobData type (projectId, keywordId?, platform?) | 1h | âœ… |
| DC-ADS-004 | Implement fetch keywords from Project's Keyword table | 1h | âœ… |
| DC-ADS-005 | Implement Meta Ads Library crawl using SearchAPI (keyword-based) | 3h | âœ… |
| DC-ADS-006 | Implement TikTok Ads Library crawl using SearchAPI (keyword-based) | 3h | âœ… |
| DC-ADS-007 | Implement Google Ads Library crawl using SearchAPI (domain-based) | 3h | âœ… |
| DC-ADS-008 | Save ads metadata to database (dedicated Ad model) | 2h | âœ… |
| DC-ADS-009 | Link ads to Project and Keyword (if keywordId provided) | 1h | âœ… |
| DC-ADS-010 | Emit ads.crawled event for downstream processing | 1h | âœ… |
| DC-ADS-011 | Update SchedulerService for scheduled ads crawl (every 4h) | 1h | âœ… |
| DC-ADS-012 | Add API endpoint for manual ads crawl trigger | 1h | âœ… |

### 4.7b Ads Library Crawler Refactor â€“ Advertiser-Centric Video Ads (DC-ADS-R-*)

> âœ… **COMPLETED**: December 2024  
> **Scope**: Refactored Ads Library crawler to be **advertiser-centric** and aligned with updated schema:
> - Use `SocialChannel.advertiserId` as the primary search key per platform.
> - Use project-level keywords (Keyword/SpyKeyword) as optional filters.
> - Creates VideoAds records directly (no intermediate Ad model).
> - Supports multiple modes: advertiserIds, socialChannelId, spyKeywordId, keywordId.

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-ADS-R-01 | Update AdsLibraryCrawlJobData to support advertiser-driven queries (link to SocialChannel IDs where needed) | 2h | âœ… |
| DC-ADS-R-02 | Implement lookup of advertisers from SocialChannel (with non-null advertiserId, competitor-linked or independent) per project | 3h | âœ… |
| DC-ADS-R-03 | Refactor Meta Ads crawl to use advertiserId + optional keyword filters | 3h | âœ… |
| DC-ADS-R-04 | Refactor TikTok Ads crawl to use advertiserId + optional keyword filters | 3h | âœ… |
| DC-ADS-R-05 | Refactor Google Ads Transparency crawl to use advertiserId instead of domain | 3h | âœ… |
| DC-ADS-R-06 | Ensure VideoAds records link back to SocialChannel/Competitor when possible (without breaking existing data) | 2h | âœ… |
| DC-ADS-R-07 | Document and enforce that only video ads are materialized into VideoAds table (not Video table) | 2h | âœ… |
| DC-ADS-R-08 | Update ads.crawled event payload description to include advertiser-centric context | 1h | âœ… |

**Implementation Notes:**
- Refactoring completed in `backend/src/modules/data-collection/processors/ads-library.processor.ts`
- Methods implemented: `processAdvertiserCrawl()`, `processSocialChannelAdvertiserCrawl()`, `processSpyKeywordCrawl()`
- VideoAds table created via migration (20251217012034_video_ads_organic_separation)
- Ads Curation workflow removed - VideoAds created directly from Ads Library APIs

#### 4.7c TikTok Ads Lock (Product Decision â€“ Dec 2025)

> **Context:** TikTok Ads Library data has been evaluated as low signal / high noise for this product phase. To control complexity and cost, TikTok ads crawling is **locked/disabled**, while Meta + Google Ads remain supported.

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-ADS-L-01 | Remove/disable TikTokâ€‘specific branches in AdsLibraryProcessor so that no new TikTok VideoAds are created | 2h | â¬œ |
| DC-ADS-L-02 | Ensure scheduler and manual triggers do not enqueue TikTok Ads crawl jobs | 1h | â¬œ |
| DC-ADS-L-03 | Update feature flags / configuration so TikTok Ads is treated as \"off\" for all environments | 1h | â¬œ |
| DC-ADS-L-04 | Verify DB: no TikTok VideoAds are being written after lock is enabled | 1h | â¬œ |

**Notes:**
- Existing TikTok VideoAds data may remain in the database for historical analysis, but no **new** records should be created after the lock.
- Frontend UI must not expose TikTokâ€‘specific filters as active options on the Video Ads page (see UI design docs).

#### 4.7d Meta Ads CTA Fields & Landing Page Discovery Enhancement (DC-ADS-CTA-*)

> **Context (Dec 2025):** Meta Ads Library response includes `cta_text`, `cta_type` (always present), and optional `link_url`. These fields must be extracted and stored in `VideoAds` records. When `link_url` is present, it represents a landing page discovered from the ad, which must be tracked as a `LandingPage` record with `discoverySource = ADS_LIBRARY`.

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-ADS-CTA-01 | Update database schema: Add `ctaText` and `ctaType` fields to `VideoAds` model | 1h | â¬œ |
| DC-ADS-CTA-02 | Update database schema: Add `discoverySource` enum and discovery tracking fields to `LandingPage` model | 2h | â¬œ |
| DC-ADS-CTA-03 | Create migration script to add new fields to `VideoAds` and `LandingPage` tables | 1h | â¬œ |
| DC-ADS-CTA-04 | Update `saveVideoAd()` method in `AdsLibraryProcessor` to extract and save `ctaText`, `ctaType`, `linkUrl` from Meta Ads response | 2h | â¬œ |
| DC-ADS-CTA-05 | Implement `handleLandingPageDiscovery()` method to create/update `LandingPage` records when `linkUrl` is present | 3h | â¬œ |
| DC-ADS-CTA-06 | Update Meta Ads extraction logic to map `snapshot.cta_text`, `snapshot.cta_type`, `snapshot.link_url` from response | 2h | â¬œ |
| DC-ADS-CTA-07 | Emit `landing-page.discovered-from-ad` event when landing page is discovered from VideoAds | 1h | â¬œ |
| DC-ADS-CTA-08 | Update VideoAds DTOs to include `ctaText`, `ctaType`, `destinationUrl` fields | 1h | â¬œ |
| DC-ADS-CTA-09 | Add validation to ensure `ctaText` and `ctaType` are always extracted for Meta Ads (log warning if missing) | 1h | â¬œ |

**Total Estimated Time:** ~14 hours (~1.75 days)

### 4.7 Scheduling (DC-SCH-*)

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-SCH-001 | Create SchedulerService with @nestjs/schedule | 2h | âœ… |
| DC-SCH-002 | Implement cron job for social crawl (daily) | 2h | âœ… |
| DC-SCH-003 | Implement cron job for store crawl (weekly) | 1h | âœ… |
| DC-SCH-004 | Implement cron job for ads crawl (4h) | 1h | âœ… |
| DC-SCH-005 | Create API endpoint for force refresh | 2h | âœ… |

### 4.8 Event Listeners (DC-EV-*)

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-EV-001 | Listen to competitor.added â†’ trigger store crawl | 1h | âœ… |
| DC-EV-002 | Listen to social-channel.added â†’ trigger channel crawl | 1h | âœ… |
| DC-EV-003 | Listen to landing-page.added â†’ trigger LP crawl | 1h | âœ… |
| DC-EV-004 | Listen to social-channel.updated â†’ trigger content crawl | 1h | âœ… |

### 4.9 Error Handling (DC-ERR-*)

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-ERR-001 | Implement exponential backoff for retries | 2h | âœ… (via BullMQ config) |
| DC-ERR-002 | Create dead letter queue for failed jobs | 2h | â¬œ (Deferred) |
| DC-ERR-003 | Implement rate limit detection and pause | 3h | â¬œ (Deferred) |
| DC-ERR-004 | Add logging for all crawl operations | 2h | âœ… |

### 4.10 Module & Controller (DC-MOD-*)

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-MOD-001 | Create DataCollectionModule | 1h | âœ… |
| DC-MOD-002 | Create CrawlController | 2h | âœ… |
| DC-MOD-003 | Register in AppModule | 0.5h | âœ… |

### 4.11 Tests (DC-TST-*)

| ID | Task | Est. | Status |
|----|------|------|--------|
| DC-TST-001 | Unit tests for ApifyAdapter | 2h | â¬œ (Phase 4) |
| DC-TST-002 | Integration tests for processors | 3h | â¬œ (Phase 4) |
| DC-TST-003 | Mock Apify responses for testing | 2h | â¬œ (Phase 4) |

---

## 5. Files to Create

```
backend/src/modules/data-collection/
â”œâ”€â”€ data-collection.module.ts           â¬œ
â”œâ”€â”€ controllers/
â”‚   â””â”€â”€ crawl.controller.ts             â¬œ
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ scheduler.service.ts            â¬œ
â”‚   â””â”€â”€ crawl-job.service.ts            â¬œ
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ store.processor.ts              â¬œ
â”‚   â”œâ”€â”€ social-channel.processor.ts     â¬œ
â”‚   â”œâ”€â”€ social-content.processor.ts     â¬œ
â”‚   â”œâ”€â”€ landing-page.processor.ts       â¬œ
â”‚   â””â”€â”€ ads-library.processor.ts       â¬œ
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ apify.adapter.ts                â¬œ
â”‚   â””â”€â”€ playwright.adapter.ts           â¬œ
â”œâ”€â”€ listeners/
â”‚   â””â”€â”€ crawl-event.listener.ts         â¬œ
â”œâ”€â”€ dto/
â”‚   â”œâ”€â”€ force-crawl.dto.ts              â¬œ
â”‚   â””â”€â”€ crawl-status.dto.ts             â¬œ
â”œâ”€â”€ types/
â”‚   â””â”€â”€ crawl.types.ts                  â¬œ
â””â”€â”€ tests/
    â””â”€â”€ apify.adapter.spec.ts           â¬œ
```

---

## 6. Queue Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  REDIS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  crawl-queue                                 â”‚
â”‚  â”œâ”€â”€ Job: store-crawl                       â”‚
â”‚  â”œâ”€â”€ Job: social-channel-crawl              â”‚
â”‚  â”œâ”€â”€ Job: social-content-crawl              â”‚
â”‚  â”œâ”€â”€ Job: landing-page-crawl                â”‚
â”‚  â””â”€â”€ Job: ads-crawl                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  dead-letter-queue (failed jobs)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. Verification Checklist

- [ ] BullMQ queue processing jobs
- [ ] Apify adapter making API calls
- [ ] Store metadata being fetched and saved
- [ ] Social channels data collected
- [ ] Videos and posts collected
- [ ] Snapshots created for time-series
- [ ] Events emitting to downstream
- [ ] Scheduled jobs running on time
- [ ] Failed jobs retrying with backoff
- [ ] Rate limits detected and respected
- [ ] Ads Library crawler fetching ads using ASO keywords
- [ ] Ads metadata saved to database
- [ ] Scheduled ads crawl running every 4h

---

## 8. Implementation Status

**âœ… COMPLETED: Ads Library Crawler (DC-ADS-001 to DC-ADS-012)**

The Ads Library crawler has been implemented with the following features:
- Uses ASO keywords from Project's Keyword table to search for competitor ads
- Crawls Meta (Facebook/Instagram), TikTok, and Google Ads Library via SearchAPI
- Saves ads metadata to dedicated `Ad` model in database
- Runs on scheduled basis (every 4 hours) and supports manual triggers via API

**Files Created/Modified:**
- `backend/prisma/schema.prisma` - Added `Ad`, `AdPlatform`, `AdStatus` models
- `backend/src/modules/data-collection/processors/ads-library.processor.ts` - New processor
- `backend/src/modules/data-collection/types/crawl.types.ts` - Added `ADS_LIBRARY_CRAWL`, `AdsLibraryCrawlJobData`
- `backend/src/modules/data-collection/processors/crawl.processor.ts` - Added ads crawl handler
- `backend/src/modules/data-collection/services/crawl-job.service.ts` - Added `addAdsLibraryCrawlJob`
- `backend/src/modules/data-collection/services/scheduler.service.ts` - Updated with new scheduler
- `backend/src/modules/data-collection/controllers/crawl.controller.ts` - Added `POST /crawl/ads/:projectId`
- `backend/src/modules/data-collection/dto/trigger-ads-crawl.dto.ts` - New DTO
- `backend/src/modules/data-collection/data-collection.module.ts` - Registered `AdsLibraryProcessor`

**API Endpoint:**
- `POST /api/crawl/ads/:projectId` - Trigger manual ads crawl for a project

**Next Steps:** Proceed to Data Processing Backend or Frontend Ads UI.

